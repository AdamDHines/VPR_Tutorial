{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Place Recognition: A Tutorial\n",
    "This notebook shows a practical code example in Python that illustrates to prospective practitioners and researchers how VPR is implemented and evaluated. The example implements a basic VPR pipeline with the key steps and components that are part of most VPR pipelines.\n",
    "\n",
    "For details or if you use our work for your academic research, please refer to the following paper:\n",
    "```bibtex\n",
    "@article{SchubertVisual,\n",
    "    title={Visual Place Recognition: A Tutorial},\n",
    "    author={Stefan Schubert and Peer Neubert and Sourav Garg and Michael Milford and Tobias Fischer},\n",
    "    journal={arXiv 2303.03281},\n",
    "    year={2023},\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Import the required libraries and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_extraction.feature_extractor_local import DELF\n",
    "from feature_extraction.feature_extractor_holistic import AlexNetConv3Extractor\n",
    "from evaluation.metrics import createPR, recallAt100precision, recallAtK\n",
    "from evaluation import show_correct_and_wrong_matches\n",
    "from matching import matching\n",
    "from datasets.load_dataset import GardensPointDataset\n",
    "import numpy as np\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual, interactive\n",
    "from IPython.display import display\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "In this example, the input to a VPR algorithm are two image sets: the database DB and query Q (i.e., multi-set VPR). For later evaluation, we also load ground-truth information about correspondences. This ground truth only serves for evaluation and will neither be available nor required when deploying the algorithm.\n",
    "\n",
    "For demonstration, the relatively small *GardensPoint Walking* dataset with 200 images per image set is loaded. *day_right* is used as database, *night_right* as query set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Load dataset GardensPoint day_right--night_right\n"
     ]
    }
   ],
   "source": [
    "# load dataset GardensPoint Walking with two image sets DB (day_right) and Q (night_right) and ground truth\n",
    "dataset = GardensPointDataset()\n",
    "imgs_db, imgs_q, GThard, GTsoft = dataset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptor computation\n",
    "The main source of information about image correspondences are image descriptors. Local descriptors like DELF provide information for multiple regions of interest but are computationally expensive to compare. Aggregation in a single holistic descriptor vector per image helps to reduce the computational complexity.\n",
    "\n",
    "In the following, a holistic image descriptor can be selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10060819ce4f47cb9ac5b99a2977432f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Descriptor', options=('HDC-DELF', 'AlexNet-conv3'), value='HDC-DELâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Widget for selecting a descriptor\n",
    "def select_descriptor(Descriptor=['HDC-DELF', 'AlexNet-conv3']):\n",
    "    return Descriptor\n",
    "w = interactive(select_descriptor)\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 12:25:14.945295: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-11 12:25:14.985677: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-11 12:25:14.986265: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-11 12:25:15.673194: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-05-11 12:25:18.762755: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'pca_dim' with dtype int32\n",
      "\t [[{{node pca_dim}}]]\n",
      "2023-05-11 12:25:18.764634: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'pca_dim' with dtype int32\n",
      "\t [[{{node pca_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Compute reference set descriptors\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009558439254760742,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 200,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d330df5e2b46c7a774b00e75326d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Compute query set descriptors\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03453707695007324,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 200,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b2f98261d04e01a26127d9ae4f8762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# select descriptor\n",
    "feature_extractor = None\n",
    "if w.result == 'HDC-DELF':\n",
    "    feature_extractor = DELF()\n",
    "elif w.result == 'AlexNet-conv3':\n",
    "    feature_extractor = AlexNetConv3Extractor()\n",
    "\n",
    "# compute holistic descriptors\n",
    "print('===== Compute reference set descriptors')\n",
    "db_D_holistic = feature_extractor.compute_features(imgs_db)\n",
    "print('===== Compute query set descriptors')\n",
    "q_D_holistic = feature_extractor.compute_features(imgs_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptor comparison and similarity matrix S\n",
    "To compare database and query descriptors, we can use their cosine similarity (e.g. computed by the inner product of the normalized descriptor vectors). Although we might not want to compute the full similarity matrix S of all possible pairs in a large scale practical application, it can be useful for visual inspection purposes.\n",
    "\n",
    "Here, the *cosine similarity* between all holistic HDC-DELF descriptors is used to compute the similarity matrix S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare all descriptors using cosine similarity\n",
    "# normalize descriptors\n",
    "db_D_holistic = db_D_holistic / np.linalg.norm(db_D_holistic , axis=1, keepdims=True)\n",
    "q_D_holistic = q_D_holistic / np.linalg.norm(q_D_holistic , axis=1, keepdims=True)\n",
    "\n",
    "# dot product between descriptors using matrix multiplication\n",
    "S = np.matmul(db_D_holistic , q_D_holistic.transpose())\n",
    "\n",
    "# show similarity matrix S\n",
    "fig = plt.imshow(S)\n",
    "fig.axes.get_xaxis().set_visible(False)\n",
    "fig.axes.get_yaxis().set_visible(False)\n",
    "fig.axes.set_title('Similarity matrix S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image shows the similarity matrix $S{\\in}\\mathbb{R}^{|DB|\\times|Q|}$. In the GardensPoint Walking dataset, images with the same ID were recorded at same places, i.e., the i-th database image matches the i-th query image. Therefore, we can observe high similarities along the main diagonal of S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image matching\n",
    "The output of a VPR pipeline is typically a set of discrete matchings, i.e. pairs of query and database images. To obtain matchings for a query image from the similarity matrix, we can either find the single best matching database image (M1) or try to find all images in the database that show the same place as the query image (M2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best matching per query in S for single-best-match VPR\n",
    "M1 = matching.best_match_per_query(S)\n",
    "\n",
    "# find matches with S>=thresh using an auto-tuned threshold for multi-match VPR\n",
    "M2 = matching.thresholding(S, 'auto')\n",
    "TP = np.argwhere(M2 & GThard)  # true positives\n",
    "FP = np.argwhere(M2 & ~GTsoft)  # false positives\n",
    "\n",
    "# show M's\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.imshow(M1)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Best match per query')\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.imshow(M2)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Thresholding S>=thresh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both matrices $M{\\in}\\mathbb{R}^{|DB|\\times|Q|}$ show the matched images between the database and the query set. Left, only the best match per query was selected, leading to a thin line. Right, all image pairs with a similarity above a threshold were selected, s.t. we can also see multiple or no matches per query image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show correct and wrong image matches\n",
    "show_correct_and_wrong_matches.show(imgs_db, imgs_q, TP, FP)  # show random matches\n",
    "plt.title('Examples for correct and wrong matches from S>=thresh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The green frame shows a correctly matched image pair, the red frame a wrongly matched image pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "To evaluate the quality of a similarity matrix S, we can apply a series of decreasing thresholds $\\theta$ to match more and more image pairs. Combined with ground-truth information, each threshold leads to a different set of true positives, false positives, true negatives and false negatives, which then provides one point on the precision-recall curve.\n",
    "\n",
    "In the following, the precision-recall curve and the area under the precision-recall curve is computed and visualized for *multi-match VPR*, i.e. all matches between each query image and the database have to be identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision-recall curve\n",
    "P, R = createPR(S, GThard, GTsoft, matching='multi', n_thresh=100)\n",
    "plt.figure()\n",
    "plt.plot(R, P)\n",
    "plt.xlim(0, 1), plt.ylim(0, 1.01)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Result on GardensPoint day_right--night_right')\n",
    "plt.grid('on')\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows the precision-recall curve. A curve closer to the upper right corner would represent better performance. Precision=1 means that no false positives (FP) were extracted. A recall=1 means that all same places were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area under precision-recall curve (AUPRC)\n",
    "AUPRC = np.trapz(P, R)\n",
    "print('\\n===== AUPRC:', AUPRC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUPRC performance ranges between 0 and 1 (higher is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# maximum recall at 100% precision\n",
    "maxR = recallAt100precision(S, GThard, GTsoft, matching='multi', n_thresh=100)\n",
    "print(f'\\n===== R@100P (maximum recall at 100% precision): {maxR:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum recall at 100% precision ranges between 0 and 1 (higher is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall@K\n",
    "Rat1 = recallAtK(S, GThard, GTsoft, K=1)\n",
    "Rat5 = recallAtK(S, GThard, GTsoft, K=5)\n",
    "Rat10 = recallAtK(S, GThard, GTsoft, K=10)\n",
    "print(f'\\n===== recall@K (R@K): R@1: {Rat1:.3f}, R@5: {Rat5:.3f}, R@10: {Rat10:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall@K ranges between 0 and 1 (higher is better). The recall@K measures the rate of query images with at least one actually matching database image. Accordingly, the metric gets better with increasing K."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
